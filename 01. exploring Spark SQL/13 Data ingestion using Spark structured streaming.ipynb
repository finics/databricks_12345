{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07150246-b3a0-4bae-a35f-8dbe33fc676e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    " \n",
    "CRIME_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"code\", StringType()),\n",
    "        StructField(\"region\", StringType()),\n",
    "        StructField(\"category\", StringType()),\n",
    "    ]\n",
    ")\n",
    " \n",
    "input_df = (\n",
    "    spark.readStream.format(\"csv\")\n",
    "    .schema(CRIME_SCHEMA)\n",
    "    .load(\n",
    "        \"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/input/\"\n",
    "    )\n",
    ")\n",
    " \n",
    "result_df = input_df.filter(col(\"region\") == \"Downtown\")\n",
    " \n",
    "result_df.writeStream.format(\"delta\").outputMode(\"append\").trigger(\n",
    "    availableNow=True\n",
    ").option(\"checkpointLocation\", \"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/checkpoint\").option(\n",
    "    \"path\",\n",
    "    \"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/output/\",\n",
    ").start().awaitTermination()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1616feb-ad7e-4920-a97c-a513c69c9046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "COPY INTO '/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/output/'\n",
    "FROM '/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/input/'\n",
    "FILEFORMAT = CSV\n",
    "PATTERN = '.*'\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "WHERE region = 'Downtown'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a11a24-0432-4d8c-ad7a-a0637050b9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    " \n",
    "CRIME_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"code\", StringType()),\n",
    "        StructField(\"region\", StringType()),\n",
    "        StructField(\"category\", StringType()),\n",
    "    ]\n",
    ")\n",
    " \n",
    "input_df = (\n",
    "    spark.readStream.format(\"csv\")\n",
    "    .schema(CRIME_SCHEMA)\n",
    "    .load(\n",
    "        \"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/input/\"\n",
    "    )\n",
    ")\n",
    " \n",
    "result_df = input_df.filter(col(\"region\") == \"Downtown\")\n",
    " \n",
    "result_df.writeStream.format(\"delta\").outputMode(\"append\").trigger(\n",
    "    availableNow=True\n",
    ").option(\n",
    "    \"checkpointLocation\",\n",
    "    \"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/crime_data/checkpoint\",\n",
    ").toTable(\n",
    "    \"san\"\n",
    ")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 Data ingestion using Spark structured streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
